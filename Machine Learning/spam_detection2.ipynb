{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6d3a6e",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Implementing a random forest classifier to detect spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e74010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a629a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"spambase.data\", header=None)\n",
    "X = data.drop([57], axis=1) #drop target feature and keep the rest as X\n",
    "Y = data[57] # save the target feature as Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "16ead117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_estimator 1\n",
      "gini impurity:  0.9102099927588704\n",
      "shannon i.g:  0.9225199131064447\n",
      "num_estimator 3\n",
      "gini impurity:  0.943519188993483\n",
      "shannon i.g:  0.9478638667632151\n",
      "num_estimator 5\n",
      "gini impurity:  0.943519188993483\n",
      "shannon i.g:  0.9572773352643013\n",
      "num_estimator 10\n",
      "gini impurity:  0.9536567704561911\n",
      "shannon i.g:  0.9630702389572773\n",
      "num_estimator 15\n",
      "gini impurity:  0.9608979000724113\n",
      "shannon i.g:  0.9616220130340333\n",
      "num_estimator 20\n",
      "gini impurity:  0.9652425778421434\n",
      "shannon i.g:  0.9616220130340333\n",
      "num_estimator 40\n",
      "gini impurity:  0.9695872556118754\n",
      "shannon i.g:  0.9695872556118754\n",
      "num_estimator 70\n",
      "gini impurity:  0.9710354815351194\n",
      "shannon i.g:  0.9695872556118754\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler() #Used to standardize the data \n",
    "l = [1,3,5,10,15,20,40,70]\n",
    "seeds = np.arange(20)\n",
    "for num_estimators in l: #testing different values for 'n_estimators' (the number of trees used in the forest)\n",
    "    results1 = []\n",
    "    results2 = []\n",
    "    for x in range(20): #for 20 seds\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seeds[x])\n",
    "        sc.fit(X_train)\n",
    "        X_train = pd.DataFrame(sc.transform(X_train))\n",
    "        X_test = pd.DataFrame(sc.transform(X_test))\n",
    "        model1 = RandomForestClassifier(n_estimators=num_estimators, criterion = 'gini', random_state=seeds[x])\n",
    "        model1.fit(X_train, Y_train)\n",
    "        pred = model1.predict(X_test)\n",
    "        results1.append(accuracy_score(Y_test, pred))\n",
    "        \n",
    "        model2 = RandomForestClassifier(n_estimators=num_estimators, criterion = 'entropy', random_state=seeds[x])\n",
    "        model2.fit(X_train, Y_train)\n",
    "        pred = model2.predict(X_test)\n",
    "        results2.append(accuracy_score(Y_test, pred))\n",
    "    #Take best accuracy score for each 'n_estimator'\n",
    "    best1 = max(results1)    \n",
    "    best2 = max(results2)\n",
    "    print('num_estimator', num_estimators)\n",
    "    print('gini impurity: ', best1)\n",
    "    print('shannon i.g: ', best2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe258969",
   "metadata": {},
   "source": [
    "The test accuracy increased as the parameter, n estimators (essentially the number of trees in the forest) increased. In general, each estimator is noisy and prone to overfitting with a high variance on outside data (as decision trees are). So, increasing the number of estimators reduces the overall variance and smooths out the noise created by each individual estimator. As a result, For both the Gini Impurity and Shannon I.G. trials, the best accuracy was found with n estimators = 70. However, the downside is that the code ran lot slower at runtime due to large number of estimators. At a certain point, increasing the number of estimators does not have any significant effect as each new estimator will not decrease the variance in any meaningful way, which is shown in the data by the fact that there is no increase from 40 to 70 for Shannon I.G. (and only a very slight increase for intervals between 10 and 70 estimators). One final idea to take note of was that the difference between the Gini accuracy and the Information Gain accuracy was largest for n estimators = 1, likely because this trial was most similar to a basic decision tree 1. Conversely, for the largest number of estimators (n=70), the Gini accuracy was actually higher than that of the Information gain, which could be because the variance was reduced enough to minimize the imperfections that arise when using Gini impurity (because of the higher variance for each individual tree which is affected by imbalanced probabilities). Overall, it is important to choose the optimal number of estimators when running a random forest model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
